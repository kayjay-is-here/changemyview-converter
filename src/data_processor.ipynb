{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (1.4.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2022.1)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: praw in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (7.6.1)\r\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from praw) (0.58.0)\r\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from praw) (2.3.0)\r\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from praw) (0.18.0)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from prawcore<3,>=2.1->praw) (2.28.1)\r\n",
      "Requirement already satisfied: six in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.9.24)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.11)\r\n",
      "Requirement already satisfied: python-dotenv in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: pyarrow in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (10.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/kayjaymac/opt/anaconda3/lib/python3.9/site-packages (from pyarrow) (1.21.5)\r\n"
     ]
    }
   ],
   "source": [
    "# Install any dependencies\n",
    "!pip install pandas\n",
    "!pip install praw\n",
    "!pip install python-dotenv\n",
    "!pip install pyarrow\n",
    "!pip install detoxify"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Set the head number to the amount of entries you want to load in minus one\n",
    "ENTRIES_COUNT = 100\n",
    "\n",
    "# Set the threshold for toxic comments to be removed\n",
    "TOXIC_THRESHOLD = 0.95"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from dotenv import main\n",
    "\n",
    "# Make sure you create a .env file and fill in all the necessary information in the same folder as this script!\n",
    "main.load_dotenv(join(dirname(os.path.realpath('__file__')), '.env'))\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "   client_id=os.environ.get(\"CLIENT_ID\"),\n",
    "   client_secret=os.environ.get(\"CLIENT_SECRET\"),\n",
    "   user_agent=\"CMV_Scraper\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the data\n",
    "import tarfile\n",
    "import os.path\n",
    "import json\n",
    "import re\n",
    "from bz2 import BZ2File\n",
    "from urllib import request\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fname = \"cmv.tar.bz2\"\n",
    "url = \"https://chenhaot.com/data/cmv/\" + fname\n",
    "\n",
    "# download if not exists\n",
    "if not os.path.isfile(fname):\n",
    "    f = BytesIO()\n",
    "    with request.urlopen(url) as resp, open(fname, 'wb') as f_disk:\n",
    "        data = resp.read()\n",
    "        f_disk.write(data)  # save to disk too\n",
    "        f.write(data)\n",
    "        f.seek(0)\n",
    "else:\n",
    "    f = open(fname, 'rb')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#tar = tarfile.open(fileobj=f, mode=\"r:bz2\")\n",
    "tar = tarfile.open(fileobj=f, mode=\"r\")\n",
    "\n",
    "# Extract the file we are interested in\n",
    "\n",
    "train_fname = \"op_task/train_op_data.jsonlist.bz2\"\n",
    "test_fname = \"op_task/heldout_op_data.jsonlist.bz2\"\n",
    "\n",
    "train_bzlist = tar.extractfile(train_fname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Deserialize the JSON list\n",
    "original_posts_train = [\n",
    "    json.loads(line.decode('utf-8'))\n",
    "    for line in BZ2File(train_bzlist)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_posts_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the jsonlist file into a dataframe\n",
    "#df = pd.read_json(original_posts_train, orient='list', lines=True)\n",
    "df = pd.DataFrame(original_posts_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to check if the posts still exists on reddit\n",
    "def try_get_post(post_id):\n",
    "    try:\n",
    "        submission = reddit.submission(id=post_id)\n",
    "        submission.name\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up the detoxifier model:\n",
    "from detoxify import Detoxify"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Removes > sign and the template message at the end of a message\n",
    "def cleanup_body_text(cmv_post):\n",
    "    lines = [line for line in cmv_post.splitlines()\n",
    "            if not line.lstrip().startswith(\"&gt;\")\n",
    "            and not line.lstrip().startswith(\"____\")\n",
    "            and not line.lstrip().startswith(\"So go forth and CMV, noble redditors!\")\n",
    "            and \"edit\" not in \" \".join(line.lower().split()[:2])\n",
    "            ]\n",
    "    #print(lines)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Detoxifies the given line and remove it if it's meta\n",
    "def cleanup_comment(selected_comment):\n",
    "\n",
    "    # Remove meta talk about the subreddit + toxic comments\n",
    "    lines = \"\\n\".join([line for line in selected_comment.splitlines()\n",
    "        if not re.search(r\"(?i)(Change\\smy\\sview|CMV)\", line)\n",
    "    ])\n",
    "\n",
    "    # Remove toxic comments\n",
    "    if Detoxify(\"multilingual\").predict(lines)[\"toxicity\"] > TOXIC_THRESHOLD:\n",
    "        print(lines)\n",
    "        print(\"Identified toxic comment, ignoring...\")\n",
    "        lines = \"\"\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Create the function that will be handling all the data gathering\n",
    "def get_top_comment_and_clean_data(post_id):\n",
    "    #print(post_id.lstrip(\"t3_\"))\n",
    "    last_author = \"\"\n",
    "    # Grab the post\n",
    "    submission = reddit.submission(id=post_id.lstrip(\"t3_\"))\n",
    "    #print(submission.title)\n",
    "\n",
    "    # Grab the highest rated comment on root layer\n",
    "    submission.submission_type = 'best'\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    replies = list(submission.comments)[0].replies.list()\n",
    "\n",
    "    # Just some variables\n",
    "    responses = pros = cons = []\n",
    "\n",
    "    # If the post author doesn't exist this submission was deleted (submission.deleted doesn't work)\n",
    "    if type(submission.author) == type(None):\n",
    "        last_author = \"[deleted]\"\n",
    "    else:\n",
    "        last_author = submission.author.name\n",
    "\n",
    "    is_pro_argument = False\n",
    "\n",
    "    for comment in replies:\n",
    "\n",
    "\n",
    "\n",
    "        # Sometimes for some reason duplicate entries exist\n",
    "        # Also remove automated message with \"Δ\" in it\n",
    "        if comment.body in responses:\n",
    "            #print(\"Skipping duplicate entry\")\n",
    "            continue\n",
    "\n",
    "        # If redditor object doesn't exist, the account is invalid/deleted\n",
    "        if type(comment.author) != type(None):\n",
    "            author = comment.author.name\n",
    "        else:\n",
    "            author = \"[deleted]\"\n",
    "\n",
    "        # Assume that whenever the user changes, they are countering the previous person\n",
    "        if author != last_author:\n",
    "            is_pro_argument = !is_pro_argument\n",
    "\n",
    "        if author == \"[deleted]\" or author==\"DeltaBot\":\n",
    "            #print(\"Skipping comment...\")\n",
    "            continue\n",
    "\n",
    "        comment.body = cleanup_comment(comment.body.replace(\"[deleted]\",\"\"))\n",
    "\n",
    "        # Add to the respective argument type        \n",
    "        if is_pro_argument:\n",
    "            pros.append(comment.body)\n",
    "        else:\n",
    "            cons.append(comment.body)\n",
    "        \n",
    "        last_author = comment.author.name\n",
    "        \n",
    "        # Pros = arguments for the Title of this post\n",
    "        # Cons = arguments against the title of this post\n",
    "\n",
    "        responses.append(comment.body)\n",
    "\n",
    "    return pros, cons, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Loading in {ENTRIES_COUNT} posts\")\n",
    "dataset = df.head(ENTRIES_COUNT)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the name column does some weird sh** because dataframes already have a name property, so migrate to a different column name\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset[\"post_id\"] = dataset[\"name\"]\n",
    "warnings.filterwarnings('default')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saying \"I hate black people\" is a clearly racist statement showing intolerance and prejudice. It is wrong because black people have no choice in the colour of their skin, and so insulting them for it makes you an asshole because you know that what you are mocking them for is something that they can never change and that doesn't effect who they are. \n",
      "\n",
      "Drawing Muhammad is not an inherently bad thing, because you are showing disrespect for a *belief* which can be changed at any point. \n",
      "Identified toxic comment, ignoring...\n",
      "> North Korea hacked Sony because they are so paranoid and insecure that they can't stand their leader being mocked. Some Muslims are so irrationally offended and self-important that they believe they have both the imperative and the right to kill people for drawing pictures. Neither of these examples illustrate problems with the speakers, they show the unhinged reactions of listeners.\n",
      "\n",
      "Damn. I think you just changed my view. That's what I needed, thank you.\n",
      "Identified toxic comment, ignoring...\n",
      "\n",
      " Loading entry 12/100:\n",
      "\t\"Change my mind: Apple Laptops are best for everyday use\"\n",
      "\n",
      " Loading entry 13/100:\n",
      "\t\"Change my mind: Eugenics isn't all that bad... And we don't even have to kill anyone for it\"\n",
      "\n",
      " Loading entry 14/100:\n",
      "\t\"Change my mind: Irony aside, most people that post in Change my mind are uneducated, and changing their view on one point won't solve this overall problem\"\n",
      "But convincing that same stupid person of something that changes other lives (i.e. an anti-vaxxer being convinced, via studies and data that target their specific concerns, to vaccinate their kids), is worthwhile.\n",
      "Identified toxic comment, ignoring...\n",
      "\n",
      " Loading entry 15/100:\n",
      "\t\"Change my mind: Prerequisites for courses should be replaced by a \"(strongly) suggested background\"\"\n",
      "\n",
      " Loading entry 16/100:\n",
      "\t\"Change my mind: Driving a car is insanely risky and probably the most dangerous thing you do in your everyday life.\"\n",
      "I'm from the UK too, them statistics are scary man. But fuck it, life's short anyway! \n",
      "Identified toxic comment, ignoring...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m/var/folders/q2/ts1qb7y92md1f383n8l8tvnw0000gn/T/ipykernel_24124/1403384816.py\u001B[0m in \u001B[0;36mget_top_comment_and_clean_data\u001B[0;34m(post_id)\u001B[0m\n\u001B[1;32m     77\u001B[0m             \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m         \u001B[0mcomment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbody\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcleanup_comment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcomment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbody\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"[deleted]\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m         \u001B[0;31m# Add to the respective argument type\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/q2/ts1qb7y92md1f383n8l8tvnw0000gn/T/ipykernel_24124/1403384816.py\u001B[0m in \u001B[0;36mcleanup_comment\u001B[0;34m(selected_comment)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0;31m# Remove toxic comments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0;32mif\u001B[0m \u001B[0mDetoxify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"multilingual\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlines\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"toxicity\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0.9\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlines\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Identified toxic comment, ignoring...\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/detoxify/detoxify.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, model_type, checkpoint, device, huggingface_config_path)\u001B[0m\n\u001B[1;32m    101\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"original\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mPRETRAINED_MODEL\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"cpu\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhuggingface_config_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m         self.model, self.tokenizer, self.class_names = load_checkpoint(\n\u001B[0m\u001B[1;32m    104\u001B[0m             \u001B[0mmodel_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel_type\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m             \u001B[0mcheckpoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcheckpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/detoxify/detoxify.py\u001B[0m in \u001B[0;36mload_checkpoint\u001B[0;34m(model_type, checkpoint, device, huggingface_config_path)\u001B[0m\n\u001B[1;32m     54\u001B[0m     }\n\u001B[1;32m     55\u001B[0m     \u001B[0mclass_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mchange_names\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcl\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcl\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mclass_names\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m     model, tokenizer = get_model_and_tokenizer(\n\u001B[0m\u001B[1;32m     57\u001B[0m         \u001B[0;34m**\u001B[0m\u001B[0mloaded\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"config\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"arch\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"args\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[0mstate_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mloaded\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"state_dict\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/detoxify/detoxify.py\u001B[0m in \u001B[0;36mget_model_and_tokenizer\u001B[0;34m(model_type, model_name, tokenizer_name, num_classes, state_dict, huggingface_config_path)\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0mlocal_files_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhuggingface_config_path\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m     )\n\u001B[0;32m---> 27\u001B[0;31m     tokenizer = getattr(transformers, tokenizer_name).from_pretrained(\n\u001B[0m\u001B[1;32m     28\u001B[0m         \u001B[0mhuggingface_config_path\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mmodel_type\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0mlocal_files_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhuggingface_config_path\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1773\u001B[0m                 \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1774\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1775\u001B[0;31m         return cls._from_pretrained(\n\u001B[0m\u001B[1;32m   1776\u001B[0m             \u001B[0mresolved_vocab_files\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1777\u001B[0m             \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m_from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1928\u001B[0m         \u001B[0;31m# Instantiate tokenizer.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1929\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1930\u001B[0;31m             \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minit_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0minit_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1931\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1932\u001B[0m             raise OSError(\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, vocab_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, sp_model_kwargs, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msp_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSentencePieceProcessor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msp_model_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msp_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLoad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocab_file\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvocab_file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentencepiece/__init__.py\u001B[0m in \u001B[0;36mLoad\u001B[0;34m(self, model_file, model_proto)\u001B[0m\n\u001B[1;32m    903\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mmodel_proto\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLoadFromSerializedProto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_proto\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 905\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLoadFromFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    906\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    907\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentencepiece/__init__.py\u001B[0m in \u001B[0;36mLoadFromFile\u001B[0;34m(self, arg)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mLoadFromFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 310\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_sentencepiece\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSentencePieceProcessor_LoadFromFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    312\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_EncodeAsIds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menable_sampling\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnbest_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madd_bos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madd_eos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreverse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memit_unk_piece\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reset variables for if we run this multiple times\n",
    "all_pros = []\n",
    "all_names = []\n",
    "all_titles = []\n",
    "all_sources = []\n",
    "\n",
    "# load in our data. this will take a while.\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "\n",
    "    post = dataset.iloc[i]\n",
    "    modified_title = post.title.replace('CMV', \"Change my mind\")\n",
    "    print(f\"\\n Loading entry {i+1}/{dataset.shape[0]}:\\n\\t\\\"{modified_title}\\\"\")\n",
    "\n",
    "    if type(post) == type(None):\n",
    "        continue\n",
    "\n",
    "    assert(post.post_id != i)\n",
    "\n",
    "    pros, _cons, _response = get_top_comment_and_clean_data(post.post_id)\n",
    "\n",
    "    # if type(post.name) == int:\n",
    "    #     continue\n",
    "    # if type(pros) == int:\n",
    "    #     continue\n",
    "    if post.title == \"[deleted]\":\n",
    "        continue\n",
    "\n",
    "    pros = \" \".join(pros)\n",
    "    pros = pros.replace(\"[deleted]\",\"\")\n",
    "\n",
    "    post.selftext = cleanup_body_text(post.selftext)\n",
    "    all_titles.append(modified_title + \" \" + post.selftext)\n",
    "    all_pros.append(pros)\n",
    "    all_names.append(post.name)\n",
    "    all_sources.append(f\"https://reddit.com/r/changemyview/comments/{post.post_id}\")\n",
    "    #print(post.title)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Place it all into a Pandas Dataframe\n",
    "clean_df = pd.DataFrame({\n",
    "    \"INSTRUCTION\": all_titles,\n",
    "    \"RESPONSE\": all_pros,\n",
    "    \"SOURCE\": all_sources\n",
    "}, index=all_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean_df.head(9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create Apache Paquete file\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pa.Table.from_pandas(clean_df)\n",
    "pq.write_table(table,\"output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test to see if it was sucessful\n",
    "table = pq.read_table(\"output.parquet\")\n",
    "table.to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
